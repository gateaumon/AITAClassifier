{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 6631,
     "status": "ok",
     "timestamp": 1603915442151,
     "user": {
      "displayName": "Sunny Vanessa Woldenga",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiZ2c5kW8lr_BvGjCQFzvT_1SC_YmcxACBtulIu7A=s64",
      "userId": "13749091948572684080"
     },
     "user_tz": 240
    },
    "id": "bbbAtz5g0uKR"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext import data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn import metrics\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "# settings to make results more reproducible\n",
    "torch.manual_seed(416)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 145214,
     "status": "ok",
     "timestamp": 1603915588258,
     "user": {
      "displayName": "Sunny Vanessa Woldenga",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiZ2c5kW8lr_BvGjCQFzvT_1SC_YmcxACBtulIu7A=s64",
      "userId": "13749091948572684080"
     },
     "user_tz": 240
    },
    "id": "yir6TnqH4N9p",
    "outputId": "9c4cce6f-22fa-465f-d31b-4d4489aa3c41"
   },
   "outputs": [],
   "source": [
    "# specify column properties for text and labels\n",
    "TEXT = data.Field(tokenize='spacy', \n",
    "                  batch_first=True, \n",
    "                  include_lengths=True, \n",
    "                  lower=True)\n",
    "LABELS = data.LabelField(batch_first=True)\n",
    "\n",
    "# only load the submission text and the according judgement for this project\n",
    "fields = {'selftext': ('text', TEXT), 'link_flair_text': ('label', LABELS)}\n",
    "\n",
    "# load the data\n",
    "train, valid, test = data.TabularDataset.splits(path='/content/', \n",
    "                                                train='train.csv', \n",
    "                                                validation='valid.csv', \n",
    "                                                test='test.csv',\n",
    "                                                format='csv', \n",
    "                                                fields=fields)\n",
    "# view the first example to make sure it was loaded and processed correctly\n",
    "print(vars(train.examples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 439992,
     "status": "ok",
     "timestamp": 1603916040378,
     "user": {
      "displayName": "Sunny Vanessa Woldenga",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiZ2c5kW8lr_BvGjCQFzvT_1SC_YmcxACBtulIu7A=s64",
      "userId": "13749091948572684080"
     },
     "user_tz": 240
    },
    "id": "zrrGI-TKA_7k",
    "outputId": "68a2d9fd-7013-40d0-856e-0942fbb8cb9a"
   },
   "outputs": [],
   "source": [
    "# build vocab for the text data, including UNK and padding tokens\n",
    "# use pre-trained embeddings to reduce overfitting\n",
    "TEXT.build_vocab(train, min_freq=3, vectors=\"glove.6B.50d\")\n",
    "\n",
    "# build label vocab\n",
    "LABELS.build_vocab(train)\n",
    "\n",
    "# info on size of vocab and classes\n",
    "print(\"Vocab size:\",len(TEXT.vocab))\n",
    "print(\"Number of classes:\",len(LABELS.vocab))\n",
    "\n",
    "# show the vocab dict and class labels\n",
    "print(TEXT.vocab.stoi)\n",
    "print(LABELS.vocab.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 338,
     "status": "ok",
     "timestamp": 1603916481766,
     "user": {
      "displayName": "Sunny Vanessa Woldenga",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiZ2c5kW8lr_BvGjCQFzvT_1SC_YmcxACBtulIu7A=s64",
      "userId": "13749091948572684080"
     },
     "user_tz": 240
    },
    "id": "VbV0ChGqMgXV"
   },
   "outputs": [],
   "source": [
    "# check if cuda is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# choose batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# create iterators for each split\n",
    "train_iter, valid_iter, test_iter = data.BucketIterator.splits(\n",
    "    (train, valid, test),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True,\n",
    "    shuffle=True,\n",
    "    device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 277,
     "status": "ok",
     "timestamp": 1603916487194,
     "user": {
      "displayName": "Sunny Vanessa Woldenga",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiZ2c5kW8lr_BvGjCQFzvT_1SC_YmcxACBtulIu7A=s64",
      "userId": "13749091948572684080"
     },
     "user_tz": 240
    },
    "id": "rFNlK2undbL0"
   },
   "outputs": [],
   "source": [
    "class AITAClassifier(nn.Module):\n",
    "    \"\"\"Bidirectional LSTM classifier\n",
    "    Parameters:\n",
    "        embedding_dim: dimensionality of the word embeddings\n",
    "        hidden_dim: dimensionality of the hidden state of the LSTM\n",
    "        vocab_size: number of unique tokens in the input data\n",
    "        num_classes: the number of classes\n",
    "        num_layers: the number of recurrent layers\n",
    "        dropout: dropout probability\n",
    "    \n",
    "    Attributes:\n",
    "        embeddings: word embeddings for all words in the vocabulary\n",
    "        lstm: bidirectional LSTM nettwork\n",
    "        hidden2tag: map from the hidden state of lstm to the tag space\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, num_classes,\n",
    "                 num_layers, dropout):\n",
    "        super(AITAClassifier, self).__init__()\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_dim, \n",
    "                            hidden_dim, \n",
    "                            num_layers=num_layers, \n",
    "                            batch_first=True,\n",
    "                            bidirectional=True,\n",
    "                            dropout=dropout)\n",
    "        \n",
    "        # hidden_dim * 2 since the network is bidirectional\n",
    "        self.hidden2tag = nn.Linear(hidden_dim * 2, num_classes)\n",
    "    \n",
    "    def forward(self, texts, text_lengths):\n",
    "        \"\"\"Carries out the forward pass of the input texts\n",
    "        Args:\n",
    "            texts: docs represented as lists of word IDs\n",
    "            text_lengths: length in words of each doc, for pad/packing\n",
    "        Returns:\n",
    "            The log probability distribution for each class for each doc\n",
    "        \"\"\"\n",
    "        embeds = self.word_embeddings(texts)\n",
    "        packed_embeds = nn.utils.rnn.pack_padded_sequence(embeds, \n",
    "                                                          text_lengths, \n",
    "                                                          batch_first=True)\n",
    "        packed_out, (hidden, cell) = self.lstm(packed_embeds)\n",
    "        # dimensions of hidden: \n",
    "        #    (num_layers * num_directions, batch size, hidden dim)\n",
    "\n",
    "        # get the forward and backward hidden states.\n",
    "        # in a bidirectional network with >1 layer, these will be the last two\n",
    "        # layers\n",
    "        hidden_forward = hidden[-2,:,:]\n",
    "        hidden_backward = hidden[-1,:,:]\n",
    "\n",
    "        # concatenate them\n",
    "        hidden = torch.cat((hidden_forward, hidden_backward), dim = 1)\n",
    "\n",
    "        class_space = self.hidden2tag(hidden)\n",
    "\n",
    "        class_scores = F.log_softmax(class_space, dim=1)\n",
    "\n",
    "        return class_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 344,
     "status": "ok",
     "timestamp": 1603916919451,
     "user": {
      "displayName": "Sunny Vanessa Woldenga",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiZ2c5kW8lr_BvGjCQFzvT_1SC_YmcxACBtulIu7A=s64",
      "userId": "13749091948572684080"
     },
     "user_tz": 240
    },
    "id": "Ws4Aq_hJBGgc",
    "outputId": "cfb78748-1e12-4f2e-f294-93a20aaf72b9"
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "EMBEDDING_DIM = 50\n",
    "HIDDEN_DIM = 32\n",
    "VOCAB_SIZE = len(TEXT.vocab)\n",
    "NUM_CLASSES = len(LABELS.vocab)\n",
    "NUM_LAYERS = 1\n",
    "DROPOUT = 0\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "# instatiate the model\n",
    "model = AITAClassifier(EMBEDDING_DIM, HIDDEN_DIM, VOCAB_SIZE, NUM_CLASSES,\n",
    "                       NUM_LAYERS, DROPOUT)\n",
    "\n",
    "# initialize pretrained embeddings\n",
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "model.word_embeddings.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "# define the loss function and optimizer\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# push to cuda if available\n",
    "model.to(device)\n",
    "loss_function.to(device)\n",
    "\n",
    "# view the model architecture\n",
    "print(model)\n",
    "\n",
    "def print_metrics(y_pred, y_true):\n",
    "    \"\"\"Prints a summary of precision, recall, and F1 for each class and the\n",
    "        macro average of all classes\n",
    "    Args:\n",
    "        y_pred: list of class predictions given by model\n",
    "        y_true: list of gold standard classes\n",
    "    \"\"\"\n",
    "    mdict = metrics.classification_report(y_true, y_pred, output_dict=True,\n",
    "                                          target_names=LABELS.vocab.itos,\n",
    "                                          zero_division=0)\n",
    "    for label in LABELS.vocab.itos:\n",
    "        print(f\"    {label:>15}  \" \n",
    "              f\"{mdict[label]['precision']:>9.2f}  \" \n",
    "              f\"{mdict[label]['recall']:>6.2f}  \" \n",
    "              f\"{mdict[label]['f1-score']:>8.2f}  \" \n",
    "              f\"{mdict[label]['support']:>7}\")\n",
    "    print(f\"          macro-avg  \" \n",
    "          f\"{mdict['macro avg']['precision']:>9.2f}  \" \n",
    "          f\"{mdict['macro avg']['recall']:>6.2f}  \" \n",
    "          f\"{mdict['macro avg']['f1-score']:>8.2f}  \"\n",
    "          f\"{mdict['macro avg']['support']:>7}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 371066,
     "status": "ok",
     "timestamp": 1603917292854,
     "user": {
      "displayName": "Sunny Vanessa Woldenga",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiZ2c5kW8lr_BvGjCQFzvT_1SC_YmcxACBtulIu7A=s64",
      "userId": "13749091948572684080"
     },
     "user_tz": 240
    },
    "id": "-EBpaNq3MKFi",
    "outputId": "b4787481-426b-4933-d6de-93227a118bb2"
   },
   "outputs": [],
   "source": [
    "def get_train_loss(model, train_iter, optimizer, loss_function):\n",
    "    \"\"\"Do a forward pass over the training data and get the loss\n",
    "    Args:\n",
    "        train_iter: iterator for the training data\n",
    "        optimizer: optimizer to use\n",
    "        loss_function: loss function to use\n",
    "    Returns;\n",
    "        The average loss for the training data\n",
    "    \"\"\"\n",
    "    # initilize loss to 0\n",
    "    epoch_loss = 0\n",
    "\n",
    "    # initilize lists for predictions and gold standard for metrics purposes\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    # set model to train\n",
    "    model.train()\n",
    "\n",
    "    for batch in train_iter:\n",
    "        # zero out the gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # get texts and corresponding lengths\n",
    "        texts, text_lengths = batch.text\n",
    "\n",
    "        # get predictions and gold standard for batch\n",
    "        class_scores = model(texts, text_lengths)\n",
    "        tags = [torch.argmax(x).item() for x in class_scores]\n",
    "        y_pred.extend(tags)\n",
    "        y_true.extend(batch.label.tolist())\n",
    "\n",
    "        # compute loss, gradients, and update parameters\n",
    "        batch_loss = loss_function(class_scores, batch.label)\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += batch_loss.item()\n",
    "    \n",
    "    # average the loss across batches\n",
    "    average_loss = epoch_loss / len(train_iter)\n",
    "\n",
    "    # print report on loss, precision, recall, and F1\n",
    "    print(f'  TRAIN loss={average_loss:.3f}' \n",
    "          '   precision  recall  f1-score  support')\n",
    "    print_metrics(y_pred, y_true)\n",
    "\n",
    "    return average_loss\n",
    "\n",
    "def get_valid_loss(model, valid_iter, loss_function):\n",
    "    \"\"\"Do a forward pass over the validation data and get the loss\n",
    "    Args:\n",
    "        valid_iter: iterator for the validation data\n",
    "        optimizer: optimizer to use\n",
    "        loss_function: loss function to use\n",
    "    Returns;\n",
    "        The average loss for the validation data\n",
    "    \"\"\"\n",
    "    # initilize loss to 0\n",
    "    epoch_loss = 0\n",
    "\n",
    "    # initilize lists for predictions and gold standard for metrics purposes\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    # set model to eval\n",
    "    model.eval()\n",
    "\n",
    "    # don't calculate gradients\n",
    "    with torch.no_grad():\n",
    "        for batch in valid_iter:\n",
    "            # get texts and corresponding lengths\n",
    "            texts, text_lengths = batch.text\n",
    "\n",
    "            # get predictions and gold standard for batch\n",
    "            class_scores = model(texts, text_lengths)\n",
    "            tags = [torch.argmax(x).item() for x in class_scores]\n",
    "            y_pred.extend(tags)\n",
    "            y_true.extend(batch.label.tolist())\n",
    "\n",
    "            # compute the loss\n",
    "            batch_loss = loss_function(class_scores, batch.label)\n",
    "            epoch_loss += batch_loss.item()\n",
    "    \n",
    "    # average the loss across batches\n",
    "    average_loss = epoch_loss / len(train_iter)\n",
    "\n",
    "    # print report on loss, precision, recall, and F1\n",
    "    print(f'\\n  VALID loss={average_loss:.3f}' \n",
    "          '   precision  recall  f1-score  support')\n",
    "    print_metrics(y_pred, y_true)\n",
    "\n",
    "    return average_loss\n",
    "\n",
    "def train_model(model, train_iter, valid_iter, optimizer, loss_function, \n",
    "                num_epochs):\n",
    "    \"\"\"Train the given model\n",
    "    Args:\n",
    "        model: neural net model to train\n",
    "        train_iter: iterator for the training data\n",
    "        valid_iter: iterator for the validation data\n",
    "        optimizer: optimizer to use\n",
    "        loss_function: loss function to use\n",
    "        num_epochs: number of epochs to train for\n",
    "    \"\"\"\n",
    "    # initilize best validation loss to infinity\n",
    "    best_valid_loss = float(\"inf\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('---------------------------------------------------------')\n",
    "        print(f'EPOCH {epoch+1}\\n')\n",
    "\n",
    "        # get the loss for training and validation sets\n",
    "        train_loss = get_train_loss(model, train_iter, optimizer, loss_function)\n",
    "        valid_loss = get_valid_loss(model, valid_iter, loss_function)\n",
    "\n",
    "        # save model parameters if loss decreased on the validation set\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), 'model_weights.pt')\n",
    "\n",
    "# train the model\n",
    "train_model(model, train_iter, valid_iter, optimizer, loss_function, NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4866,
     "status": "ok",
     "timestamp": 1603917622464,
     "user": {
      "displayName": "Sunny Vanessa Woldenga",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiZ2c5kW8lr_BvGjCQFzvT_1SC_YmcxACBtulIu7A=s64",
      "userId": "13749091948572684080"
     },
     "user_tz": 240
    },
    "id": "gHhjbo7Wo-Kq",
    "outputId": "e6cc7528-1e8a-45fc-d414-d18c70c1772c"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, test_iter):\n",
    "    \"\"\"Evaluate the model on the test set\n",
    "    Args:\n",
    "        model: neural net model to predict with\n",
    "        test_iter: iterator for the test data\n",
    "    \"\"\"\n",
    "    # initilize lists for predictions and gold standard for metrics purposes\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    # set model to eval\n",
    "    model.eval()\n",
    "\n",
    "    # don't calculate gradients\n",
    "    with torch.no_grad():\n",
    "        for batch in test_iter:\n",
    "            # get texts and corresponding lengths\n",
    "            texts, text_lengths = batch.text\n",
    "\n",
    "            # get predictions and gold standard for batch\n",
    "            class_scores = model(texts, text_lengths)\n",
    "            tags = [torch.argmax(x).item() for x in class_scores]\n",
    "            y_pred.extend(tags)\n",
    "            y_true.extend(batch.label.tolist())\n",
    "    \n",
    "    # print the classification report for the data\n",
    "    print(metrics.classification_report(y_true, y_pred, \n",
    "                                        target_names=LABELS.vocab.itos,\n",
    "                                        zero_division=0))\n",
    "\n",
    "# load the saved model parameters\n",
    "model.load_state_dict(torch.load('/content/model_weights.pt'))\n",
    "\n",
    "# evaluate the model\n",
    "evaluate(model, test_iter)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOvcVmZq0t2dyWPPK7pcgZV",
   "name": "AITA_Classifier.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

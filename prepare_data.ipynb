{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "from spacy.lang.en import English\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# seed for sklearn\n",
    "SEED = 416\n",
    "\n",
    "# NLP object used for tokenization\n",
    "nlp = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define class labels\n",
    "YTA = 'asshole'\n",
    "NTA = 'not the a-hole'\n",
    "ESH = 'everyone sucks'\n",
    "NAH = 'no a-holes here'\n",
    "FLAIRS = [YTA, NTA, ESH, NAH]\n",
    "\n",
    "# define the dtype for each column of the csv\n",
    "DTYPES = {'id': str, \n",
    "          'author': str, \n",
    "          'created_utc': float, \n",
    "          'num_comments': int, \n",
    "          'over_18': bool, \n",
    "          'selftext': str, \n",
    "          'title': str, \n",
    "          'link_flair_text': str}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# include dtypes so that each column is read correctly\n",
    "submissions = pd.read_csv('data/reddit_raw.csv', dtype=DTYPES)\n",
    "\n",
    "# get rid of rows with NaN\n",
    "submissions = submissions.dropna(axis=0, how='any')\n",
    "submissions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowercase the flairs\n",
    "submissions['link_flair_text'] = submissions['link_flair_text'].str.lower()\n",
    "\n",
    "# have only one label for each kind of verdict\n",
    "nah_replace_flairs = ['no assholes here', 'no a--holes here']\n",
    "nta_replace_flair = 'not the asshole'\n",
    "submissions.loc[submissions['link_flair_text'].isin(nah_replace_flairs), 'link_flair_text'] = NAH\n",
    "submissions.loc[submissions['link_flair_text'] == nta_replace_flair, 'link_flair_text'] = NTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_class_distribution(df):\n",
    "    \"\"\"Prints the submission count and percentage for each class of the input df\n",
    "    Args:\n",
    "        df: dataframe to get class distribution for\n",
    "    \"\"\"\n",
    "    assholes = len(df[df['link_flair_text'] == YTA])\n",
    "    not_assholes = len(df[df['link_flair_text'] == NTA])\n",
    "    everyone_sucks = len(df[df['link_flair_text'] == ESH])\n",
    "    no_assholes = len(df[df['link_flair_text'] == NAH])\n",
    "    total = len(df)\n",
    "    print(f'YTA: {assholes: >6} submissions, {((assholes / total) * 100):.1f}%')\n",
    "    print(f'NTA: {not_assholes: >6} submissions, {((not_assholes / total) * 100):.1f}%')\n",
    "    print(f'ESH: {everyone_sucks: >6} submissions, {((everyone_sucks / total) * 100):.1f}%')\n",
    "    print(f'NAH: {no_assholes: >6} submissions, {((no_assholes / total) * 100):.1f}%')\n",
    "    print(f'Total: {total} submissions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['asshole' 'everyone sucks' 'not the a-hole' 'no a-holes here']\n",
      "YTA:  42042 submissions, 20.8%\n",
      "NTA: 125610 submissions, 62.1%\n",
      "ESH:  11916 submissions, 5.9%\n",
      "NAH:  22820 submissions, 11.3%\n",
      "Total: 202388 submissions\n"
     ]
    }
   ],
   "source": [
    "# get only the submissions with the four flairs we are interested in\n",
    "# use .copy() so changes aren't made to orginal df\n",
    "relevant_submissions = submissions.loc[submissions['link_flair_text'].isin(FLAIRS)].copy()\n",
    "\n",
    "# verify we have only selected rows with one of the four verdicts we're interested in\n",
    "print(pd.unique(relevant_submissions['link_flair_text']))\n",
    "\n",
    "# get info on our dataset\n",
    "print_class_distribution(relevant_submissions)\n",
    "\n",
    "# save to csv so I can read from here instead of the full raw data in the future\n",
    "relevant_submissions.to_csv('data/data_4_flairs.csv', \n",
    "                            index=False, sep=',', \n",
    "                            quotechar='\"', \n",
    "                            quoting=csv.QUOTE_NONNUMERIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min 15 comments: 130575 submissions\n",
      "Min 25 comments: 80276 submissions\n",
      "Min 50 comments: 38698 submissions\n",
      "Min 75 comments: 26778 submissions\n",
      "Min 100 comments: 21293 submissions\n"
     ]
    }
   ],
   "source": [
    "# Check how many submissions there are when we filter out submissions with <n comments.\n",
    "# Since r/AITA chooses a verdict based on the top comment, I want to make sure there\n",
    "# is a decent amount of activity on the post but also don't want to reduce the size of\n",
    "# my dataset too much.\n",
    "min_15_comments = relevant_submissions.loc[relevant_submissions['num_comments'] >= 15].copy()\n",
    "min_25_comments = relevant_submissions.loc[relevant_submissions['num_comments'] >= 25].copy()\n",
    "min_50_comments = relevant_submissions.loc[relevant_submissions['num_comments'] >= 50].copy()\n",
    "min_75_comments = relevant_submissions.loc[relevant_submissions['num_comments'] >= 75].copy()\n",
    "min_100_comments = relevant_submissions.loc[relevant_submissions['num_comments'] >= 100].copy()\n",
    "print(f'Min 15 comments: {len(min_15_comments)} submissions')\n",
    "print(f'Min 25 comments: {len(min_25_comments)} submissions')\n",
    "print(f'Min 50 comments: {len(min_50_comments)} submissions')\n",
    "print(f'Min 75 comments: {len(min_75_comments)} submissions')\n",
    "print(f'Min 100 comments: {len(min_100_comments)} submissions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min 15 comments:\n",
      "YTA:  33532 submissions, 25.7%\n",
      "NTA:  76407 submissions, 58.5%\n",
      "ESH:   7506 submissions, 5.7%\n",
      "NAH:  13130 submissions, 10.1%\n",
      "Total: 130575 submissions\n"
     ]
    }
   ],
   "source": [
    "# I chose submissions with a minimum of 15 comments based on the above numbers\n",
    "# The class distribution remains approximately the same as the full dataset,\n",
    "# with the proportion of YTA verdicts changing the most\n",
    "print('Min 15 comments:')\n",
    "print_class_distribution(min_15_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# make a new column containing the length (in words) of each submission's selftext\n",
    "min_15_comments['selftext_len'] = [len(nlp(x)) for x in min_15_comments['selftext'].tolist()]\n",
    "min_15_comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YTA:  31782 submissions, 25.1%\n",
      "NTA:  74740 submissions, 59.1%\n",
      "ESH:   7214 submissions, 5.7%\n",
      "NAH:  12808 submissions, 10.1%\n",
      "Total: 126544 submissions\n"
     ]
    }
   ],
   "source": [
    "# filter out posts with selftexts that are too short\n",
    "# some of these are posts with just a few sentences which I want to eliminate to give\n",
    "# the classifier more to work with, but most of them are posts which have been deleted\n",
    "# and whose selftext field has been changed to \"[removed]\" as a result\n",
    "min_50_words = min_15_comments.loc[min_15_comments['selftext_len'] >= 50].copy()\n",
    "\n",
    "# the class distribution remains approximately the same as the previous dataset after\n",
    "# filtering out ~4000 submissions\n",
    "print_class_distribution(min_50_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "YTA:  25744 submissions, 25.1%\n",
      "NTA:  60539 submissions, 59.1%\n",
      "ESH:   5843 submissions, 5.7%\n",
      "NAH:  10374 submissions, 10.1%\n",
      "Total: 102500 submissions\n",
      "Valid shape:\n",
      "YTA:   2860 submissions, 25.1%\n",
      "NTA:   6727 submissions, 59.1%\n",
      "ESH:    649 submissions, 5.7%\n",
      "NAH:   1153 submissions, 10.1%\n",
      "Total: 11389 submissions\n",
      "Test shape:\n",
      "YTA:   3178 submissions, 25.1%\n",
      "NTA:   7474 submissions, 59.1%\n",
      "ESH:    722 submissions, 5.7%\n",
      "NAH:   1281 submissions, 10.1%\n",
      "Total: 12655 submissions\n"
     ]
    }
   ],
   "source": [
    "# split data into training and test\n",
    "train_test_ratio = 0.9\n",
    "\n",
    "# Stratify my splits so that each class has the same proportion of each split\n",
    "df_train_full, df_test = train_test_split(min_50_words, \n",
    "                                          stratify=min_50_words['link_flair_text'], \n",
    "                                          train_size=train_test_ratio,\n",
    "                                          random_state=SEED)\n",
    "# NTS: if you get an error like, e.g.\n",
    "# >ValueError: The test_size = 2 should be greater or equal to the number of classes = 3\n",
    "# it's because based on the size of data you're splitting, the test split (=2) cannot have\n",
    "# enough items to represent all the classes (=3). This is due to specifying you want\n",
    "# stratified splits. You probably won't run into this when you're working with full data\n",
    "\n",
    "# split training data into training and validation\n",
    "train_valid_ratio = 0.9\n",
    "\n",
    "df_train, df_valid = train_test_split(df_train_full, \n",
    "                                      stratify=df_train_full['link_flair_text'],\n",
    "                                      train_size=train_valid_ratio,\n",
    "                                      random_state=SEED)\n",
    "# check the number of submissions in each split and verify that the splits\n",
    "# have been stratified\n",
    "print('Train:')\n",
    "print_class_distribution(df_train)\n",
    "print('Valid shape:')\n",
    "print_class_distribution(df_valid)\n",
    "print('Test shape:')\n",
    "print_class_distribution(df_test)\n",
    "\n",
    "# write the splits to files\n",
    "df_train.to_csv('data/train.csv', index=False, sep=',', quotechar='\"', quoting=csv.QUOTE_NONNUMERIC)\n",
    "df_valid.to_csv('data/valid.csv', index=False, sep=',', quotechar='\"', quoting=csv.QUOTE_NONNUMERIC)\n",
    "df_test.to_csv('data/test.csv', index=False, sep=',', quotechar='\"', quoting=csv.QUOTE_NONNUMERIC)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
